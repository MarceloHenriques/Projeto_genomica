{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d036b98-19d9-4ff6-bca6-b05878d8ddcc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Instlação\n",
    "\n",
    "* pip install --upgrade pip\n",
    "* pip install kafka-python\n",
    "* pip install newsapi-python\n",
    "* pip install schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6e98f1c-936a-45cb-bffe-13afba2d91a9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef5b831a-ae70-4037-a5da-7038d9e541ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Window\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "979fbb4d-c9c7-46f7-b76f-f1b73f76095b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Critérios de Busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a36d99a-20e0-47cf-911a-10c56f7dd296",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Search_Criteria:\n",
    "    def __init__(self, subject, keywords):\n",
    "        self.__subject = subject # \"genomics\"\n",
    "        self.__keywords = keywords # [\"DNA\", \"genetics\", \"treatment\"]\n",
    "    \n",
    "\n",
    "    # Adicionar palavra chave\n",
    "    def add_keyworkd(self, new_keyword):\n",
    "        self.__keywords.append(new_keyword)\n",
    "\n",
    "\n",
    "    # Remover palavra chave\n",
    "    def remove_keyworkd(self, old_keyword):\n",
    "        self.__keywords.pop(self.__keywords.index(old_keyword))\n",
    "\n",
    "    @property\n",
    "    def subject(self):\n",
    "        return self.__subject\n",
    "    \n",
    "    @subject.setter\n",
    "    def subject(self, new_subject):\n",
    "        self.__subject = new_subject\n",
    "\n",
    "    @property\n",
    "    def keywords(self):\n",
    "        return self.__keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dddbca02-f0dd-45da-a0d4-ba10fc8c3a2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "243c2426-e69f-4344-a9ce-0604dd2ae4a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class APIClient:\n",
    "    def __init__(self, key, query) -> None:\n",
    "        self.__client = NewsApiClient(api_key=key)\n",
    "        self.__query = query\n",
    "        self.__all_data = None\n",
    "        self.__articles = []\n",
    "\n",
    "\n",
    "    # Critérios de busca\n",
    "    def search(self, page):\n",
    "        try:\n",
    "            self.__all_data = self.__client.get_everything(q=self.__query, page = page)\n",
    "        except:\n",
    "            self.__all_data = {\"status\": \"end\"}\n",
    "\n",
    "\n",
    "    # Requerer os artigos da API\n",
    "    def get_articles(self):\n",
    "        page = 1\n",
    "        self.search(page=page)\n",
    "        while self.__all_data[\"status\"] == \"ok\":\n",
    "            self.__articles += self.__all_data[\"articles\"]\n",
    "            page += 1\n",
    "            self.search(page=page)\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def articles(self):\n",
    "        return self.__articles\n",
    "    \n",
    "    @property\n",
    "    def all_data(self):\n",
    "        return self.__all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb5addb4-1b93-44d3-8286-94f6ba05fd8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c75b561-95f9-416d-a345-91603d45df2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Data_Pipeline_API:\n",
    "    def __init__(self) -> None:\n",
    "        self.__df = None\n",
    "        self.__df_parquet = None\n",
    "\n",
    "\n",
    "    # Carregar dados da API\n",
    "    def load_api(self, data) -> None:\n",
    "        self.__df = spark.createDataFrame(data)\n",
    "        ordem = self.__df.columns\n",
    "        self.__df = self.__df.withColumn(\"source\", col(\"source\").getItem(\"name\"))\n",
    "        self.__df = self.__df.select(ordem)\n",
    "\n",
    "\n",
    "    # Transformar os dados .parquet em DataFrame\n",
    "    def load_parquet(self, name) -> None:\n",
    "        file = f\"{name}.parquet\"\n",
    "        self.__df_parquet = spark.read.parquet(f\"/FileStore/Projeto/data/{file}\")\n",
    "\n",
    "\n",
    "    # Concatenando o DataFrame com o arquivo parquet\n",
    "    # Mantém apenas colunas únicas\n",
    "    def save_data(self, name) -> None:\n",
    "        file = f\"{name}.parquet\"\n",
    "        if name == \"raw_data\":\n",
    "            ordem = \"publishedAt\"\n",
    "        else: \n",
    "            ordem = \"data_publicacao\"\n",
    "        if file in [y.name.strip() for y in dbutils.fs.ls(\"/FileStore/Projeto/data/\")] or f\"{file}/\" in [y.name.strip() for y in dbutils.fs.ls(\"/FileStore/Projeto/data/\")]:\n",
    "            self.load_parquet(name)\n",
    "            windowSpec = Window.orderBy(ordem)\n",
    "            self.__df = self.__df.unionByName(self.__df_parquet)\n",
    "            self.__df = (self.__df.withColumn(\"row_index\", row_number().over(windowSpec))\n",
    "                         .drop(\"row_index\")\n",
    "                         )\n",
    "\n",
    "            self.__df = self.__df.dropDuplicates()\n",
    "        self.__df.write.mode(\"overwrite\").parquet(f\"/FileStore/Projeto/data/{file}\")\n",
    "\n",
    "\n",
    "    # Limpeza dos dados Brutos\n",
    "    def data_cleaning(self):\n",
    "        df_order = ['data_publicacao', 'titulo', 'autor', 'descricao', 'url', 'fonte', 'conteudo']\n",
    "        windowSpec = Window.orderBy(\"data_publicacao\")\n",
    "\n",
    "        self.__df = (self.__df_parquet.drop(col(\"urlToImage\"))\n",
    "                     .withColumnRenamed(\"author\", \"autor\")\n",
    "                     .withColumnRenamed(\"title\", \"titulo\")\n",
    "                     .withColumnRenamed(\"description\", \"descricao\")\n",
    "                     .withColumnRenamed(\"publishedAt\", \"data_publicacao\")\n",
    "                     .withColumnRenamed(\"content\", \"conteudo\")\n",
    "                     .withColumnRenamed(\"source\", \"fonte\")\n",
    "                     .withColumn(\"data_publicacao\", to_date(substring(trim(col(\"data_publicacao\")), 1, 10), \"yyyy-MM-dd\"))\n",
    "                     .withColumn(\"row_index\", row_number().over(windowSpec))\n",
    "                     .drop(col(\"row_index\"))\n",
    "                    )\n",
    "        self.__df = self.__df.select(*df_order)\n",
    "    \n",
    "\n",
    "    # Salvando em Delta Table\n",
    "    def delta_table(self):\n",
    "        self.load_parquet(\"cleaned_data\")\n",
    "\n",
    "        # Salvando em Delta\n",
    "        self.__df_parquet.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/Projeto/delta\")\n",
    "\n",
    "        # Disponibilizando em banco de dados\n",
    "        self.__df_parquet.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"pipeline_delta\")\n",
    "    \n",
    "\n",
    "    def to_gold(self, keywords):\n",
    "        self.load_parquet(\"cleaned_data\")\n",
    "\n",
    "        # Agrupamento por ano, mês e dia\n",
    "        df_time_stats = self.__df_parquet.groupBy(\n",
    "            year(col(\"data_publicacao\")).alias(\"year\"),\n",
    "            month(col(\"data_publicacao\")).alias(\"month\"),\n",
    "            dayofmonth(col(\"data_publicacao\")).alias(\"day\")).count()\n",
    "\n",
    "        # Ordenar pela soma\n",
    "        df_time_stats = df_time_stats.orderBy(col(\"count\").desc())\n",
    "\n",
    "        # Disponibilizar no db\n",
    "        df_time_stats.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"artigos_ano_mes_dia\")\n",
    "\n",
    "\n",
    "        # Agrupamento por fonte e autor\n",
    "        df_source_author_stats = self.__df_parquet.groupBy(\"fonte\", \"autor\").count()\n",
    "\n",
    "        # Ordenar pela soma\n",
    "        df_source_author_stats = df_source_author_stats.orderBy(col(\"count\").desc())\n",
    "\n",
    "        # Disponibilizar no db\n",
    "        df_source_author_stats.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"artigos_fonte_autor\")\n",
    "\n",
    "\n",
    "        # Para Tratamento das palavras chave\n",
    "        df_keywords = self.__df_parquet\n",
    "        \n",
    "        # Adicionar colunas de contagem de palavras-chave (1 se a palavra aparecer, 0 caso contrário)\n",
    "        for keyword in keywords:\n",
    "            df_keywords = df_keywords.withColumn(keyword, when(col(\"conteudo\").contains(keyword), 1).otherwise(0))\n",
    "\n",
    "        # Agrupar por ano, mês e dia e somar as aparições das palavras-chave\n",
    "        df_keyword_stats = df_keywords.groupBy(\n",
    "            year(col(\"data_publicacao\")).alias(\"year\"),\n",
    "            month(col(\"data_publicacao\")).alias(\"month\")    \n",
    "        ).agg(*[sum(col(keyword)).alias(f\"sum_{keyword}\") for keyword in keywords])\n",
    "\n",
    "        # Disponibilizar no db\n",
    "        df_keyword_stats.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"artigos_palavras_chave\")\n",
    "\n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        return self.__df\n",
    "    \n",
    "    @df.setter\n",
    "    def df(self, dataframe):\n",
    "        self.__df = dataframe\n",
    "\n",
    "\n",
    "    @property\n",
    "    def df_parquet(self):\n",
    "        return self.__df_parquet"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04.objects",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
