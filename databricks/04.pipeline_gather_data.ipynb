{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3173e5ff-06a3-4ecc-8ee3-45ab179cfbcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Instlação\n",
    "\n",
    "* pip install --upgrade pip\n",
    "* pip install kafka-python\n",
    "* pip install newsapi-python\n",
    "* pip install schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac0c144c-08be-488e-8ca7-2591b7c64690",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25b9ea19-bd51-4fff-82a5-0bf1d6d5b054",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import to_date, col\n",
    "from kafka import KafkaConsumer\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab469702-5575-4952-9a35-7e2f84077144",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8700e25f-ad57-42f4-a4d0-3323a41d96f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Data_Pipeline_API:\n",
    "    def __init__(self) -> None:\n",
    "        self.__df = None\n",
    "        self.__df_parquet = None\n",
    "\n",
    "\n",
    "    # Carregar dados da API\n",
    "    def load_api(self, data) -> None:\n",
    "        self.__df = ps.DataFrame(data)\n",
    "        sources = []\n",
    "        for aux in self.__df['source'].to_numpy():\n",
    "            if isinstance(aux, dict):\n",
    "                sources.append(aux['name'])\n",
    "            else:\n",
    "                sources.append(None)\n",
    "        self.__df['source'] = sources\n",
    "\n",
    "\n",
    "    # Transformar os dados .parquet em DataFrame\n",
    "    def load_parquet(self, name) -> None:\n",
    "        file = f\"{name}.parquet\"\n",
    "        self.__df_parquet = ps.read_parquet(f\"/FileStore/Projeto/data/{file}\")\n",
    "\n",
    "\n",
    "    # Concatenando o DataFrame com o arquivo parquet\n",
    "    # Mantém apenas colunas únicas\n",
    "    def save_data(self, name) -> None:\n",
    "        file = f\"{name}.parquet\"\n",
    "        if file in [y.name.strip() for y in dbutils.fs.ls(\"/FileStore/Projeto/data/\")] or f\"{file}/\" in [y.name.strip() for y in dbutils.fs.ls(\"/FileStore/Projeto/data/\")]:\n",
    "            self.load_parquet(name)\n",
    "            self.__df = ps.concat([self.__df_parquet, self.__df]).drop_duplicates().reset_index().drop(columns=[\"index\"])\n",
    "        self.__df.to_parquet(f\"/FileStore/Projeto/data/{file}\", mode='overwrite')\n",
    "\n",
    "\n",
    "    # Limpeza dos dados Brutos\n",
    "    def data_cleaning(self):\n",
    "        # Remove coluna de url de imagem de capa\n",
    "        self.__df = self.__df_parquet.drop(columns=['urlToImage'])\n",
    "        \n",
    "        #Alterar o nome das colunas\n",
    "        self.__df = self.__df.rename(columns={'author': 'autor',\n",
    "                                              'title': 'titulo',\n",
    "                                              'description': 'descricao',\n",
    "                                              'publishedAt': 'data_publicacao',\n",
    "                                              'content': 'conteudo',\n",
    "                                              'source': 'fonte'})\n",
    "\n",
    "        # Organizar colunas\n",
    "        df_order = ['data_publicacao', 'titulo', 'autor', 'descricao', 'url', 'fonte', 'conteudo']\n",
    "        self.__df = self.__df[df_order]\n",
    "\n",
    "        # Ajuste do dia de publicação\n",
    "        data = self.__df['data_publicacao'].to_numpy()\n",
    "        date_df = pd.DataFrame(data = data)\n",
    "        date_df.columns = ['data_publicacao']\n",
    "        date_df['data_publicacao'] = pd.to_datetime(date_df['data_publicacao'])\n",
    "        date_df['data_publicacao'] = date_df['data_publicacao'].dt.strftime('%Y/%m/%d')\n",
    "        \n",
    "        # Atribuição de valores\n",
    "        for i in range(self.__df.shape[0]):\n",
    "            self.__df.loc[i, 'data_publicacao'] = date_df.loc[i, 'data_publicacao']\n",
    "\n",
    "        # Ordenar Datas\n",
    "        self.__df = self.__df.sort_values('data_publicacao').reset_index().drop(columns=['index'])\n",
    "\n",
    "\n",
    "    # Salvando em Delta Table\n",
    "    def delta_table(self):\n",
    "        df_delta = spark.read.parquet(\"/FileStore/Projeto/data/cleaned_data.parquet\")\n",
    "        df_delta = df_delta.withColumn(\"data_publicacao\", to_date(col(\"data_publicacao\"), \"yyyy/MM/dd\"))\n",
    "\n",
    "        # Salvando em Delta\n",
    "        df_delta.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/Projeto/delta\")\n",
    "\n",
    "        # Disponibilizando em banco de dados\n",
    "        df_delta.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"pipeline_delta\")\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        return self.__df\n",
    "    \n",
    "    @df.setter\n",
    "    def df(self, dataframe):\n",
    "        self.__df = dataframe\n",
    "\n",
    "\n",
    "    @property\n",
    "    def df_parquet(self):\n",
    "        return self.__df_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1836f941-4907-48e5-bbb0-66aa4bf92960",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478117f3-fd32-4d41-b93c-5585a3785b51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-898858298568835>, line 60\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    self.__df. = self.__df.withColumn(\"data_publicacao\", to_date(df[\"data_publicacao\"], \"yyyy/MM/dd\"))\u001B[0m\n",
       "\u001B[0m               ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SyntaxError",
        "evalue": "invalid syntax (command-898858298568835-281729184, line 60)"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;36m  File \u001B[0;32m<command-898858298568835>, line 60\u001B[0;36m\u001B[0m\n\u001B[0;31m    self.__df. = self.__df.withColumn(\"data_publicacao\", to_date(df[\"data_publicacao\"], \"yyyy/MM/dd\"))\u001B[0m\n\u001B[0m               ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "consumer = KafkaConsumer(\"genomics-news\", bootstrap_servers=[\"localhost:9092\"])\n",
    "\n",
    "for message in consumer:\n",
    "    now = datetime.datetime.now()\n",
    "    now_min = now.minute\n",
    "    print(f\"Processo iniciado em {now.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    value = message.value\n",
    "    articles_list = pickle.loads(value)\n",
    "\n",
    "    if len(articles_list) != 0:\n",
    "        \n",
    "        # Instanciando Pipeline\n",
    "        pipeline_data = Data_Pipeline_API()\n",
    "\n",
    "        # Carregando dados da API\n",
    "        pipeline_data.load_api(articles_list)\n",
    "\n",
    "        # Salvando em Parquet\n",
    "        pipeline_data.save_data(\"raw_data\")\n",
    "        print(f\"raw_data atualizado em {now.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2025689123752890,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04.pipeline_gather_data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
