{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a4b5150-e61f-4385-8164-351d93290d77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4- EXECUTAR SERVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6c5f47a-f0cb-425c-a6c2-9a8be59c4e8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-23 15:49:53,708] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)\n[2024-10-23 15:49:58,785] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)\n[2024-10-23 15:49:58,818] INFO RemoteLogManagerConfig values: \n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tremote.fetch.max.wait.ms = 500\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.copier.thread.pool.size = 10\n\tremote.log.manager.copy.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.copy.quota.window.num = 11\n\tremote.log.manager.copy.quota.window.size.seconds = 1\n\tremote.log.manager.expiration.thread.pool.size = 10\n\tremote.log.manager.fetch.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.fetch.quota.window.num = 11\n\tremote.log.manager.fetch.quota.window.size.seconds = 1\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)\n[2024-10-23 15:50:00,274] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)\n[2024-10-23 15:50:00,304] INFO starting (kafka.server.KafkaServer)\n[2024-10-23 15:50:00,308] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)\n[2024-10-23 15:50:00,924] INFO [ZooKeeperClient Kafka server] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)\n[2024-10-23 15:50:01,072] INFO Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT (org.apache.zookeeper.ZooKeeper)\n[2024-10-23 15:50:01,073] INFO Client environment:host.name=1023-154143-zl92n0lo-10-172-187-10 (org.apache.zookeeper.ZooKeeper)\n[2024-10-23 15:50:01,073] INFO Client environment:java.version=1.8.0_412 (org.apache.zookeeper.ZooKeeper)\n[2024-10-23 15:50:01,073] INFO Client environment:java.vendor=Azul Systems, Inc. (org.apache.zookeeper.ZooKeeper)\n[2024-10-23 15:50:01,073] INFO Client environment:java.home=/usr/lib/jvm/zulu8-ca-amd64/jre (org.apache.zookeeper.ZooKeeper)\n[2024-10-23 15:50:01,084] INFO Client environment:java.class.path=/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/log4j/driver:/databricks/hive/conf:/databricks/spark/dbconf/hadoop:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---1649304808--io.netty__netty-transport-native-kqueue__4.1.106.Final-osx-x86_64.jar:/databricks/jars/----ws_3_5--third_party--bigquery-jdbc--bigquery-driver-shaded---846918551--json-20230227.jar:/databricks/jars/third_party--armeria--armeria_shaded_2.12---578504376--com.google.j2objc__j2objc-annotations__2.8.jar:/databricks/jars/----ws_3_5--connector--connect--server--spark_connect_proto_lib_shaded-hive-2.3__hadoop-3.2_2.12---346995935--io.netty__netty-common__4.1.106.Final.jar:/databricks/jars/----ws_3_5--third_party--bigquery-jdbc--bigquery-driver-shaded---846918551--auto-value-1.10.1.jar:/databricks/jars/common--logging--metrics--metrics_util-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12--1707721562--io.netty__netty-transport-udt__4.1.94.Final.jar:/databricks/jars/zippy--zippy-sdk--common--parser-spark_3.5_2.12_deploy.jar:/databricks/jars/proto--logs--activity--accounts-manager--cloud_subscription_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/proto--logs--ingestion--connector_event_proto-jetty9-hadoop1_2.12-scalabp.jar:/databricks/jars/third_party--scalapb-090--grpc_shaded_scala_2.12--1317552945--runtime-unshaded-jetty9-hadoop1_2.12_deploy.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12--719397394--com.microsoft.azure__azure-keyvault-core__1.2.6.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---1023284099--com.google.protobuf__protobuf-java-util__3.21.9.jar:/databricks/jars/common--http--tag-definition-parser--tag-definition-parser-spark_3.5_2.12_deploy.jar:/databricks/jars/armeria--conf--enable-client-side-load-balancing-internal-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12---359997420--io.netty__netty-codec-smtp__4.1.94.Final.jar:/databricks/jars/----ws_3_5--connector--protobuf--protobuf-java-util_shaded-for-protobuf-hive-2.3__hadoop-3.2---361727721--com.google.protobuf__protobuf-java-util__3.19.4.jar:/databricks/jars/third_party--opencensus-shaded--opencensus_shaded_base--1362165133--org.checkerframework__checker-qual__3.33.0.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--io.netty--netty-transport-native-epoll--io.netty__netty-transport-native-epoll__4.1.96.Final.jar:/databricks/jars/proto--logs--product-event--repos--projects_operation_duration_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/----ws_3_5--third_party--gcp-java--gcp-java_shaded---1362152157--com.google.api__gax__2.3.0.jar:/databricks/jars/proto--logs--activity--cluster-policy--cluster_policy_activity_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/central--api--service--service-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--safespark--udf--common--udf_grpc-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----ws_3_5--third_party--gcp-java--gcp-java_shaded---413349809--io.grpc__grpc-protobuf__1.57.1.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.amazonaws--aws-java-sdk-cloudwatch--com.amazonaws__aws-java-sdk-cloudwatch__1.12.610.jar:/databricks/jars/third_party--armeria--armeria_shaded_2.12--619126430--io.perfmark__perfmark-api__0.26.0.jar:/databricks/jars/third_party--armeria--armeria_shaded_2.12---346995935--io.netty__netty-common__4.1.106.Final.jar:/databricks/jars/third_party--tink--tink-shaded_2.12--1772517849--io.grpc__grpc-protobuf-lite__1.60.0.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.github.fommil.netlib--native_ref-java--com.github.fommil.netlib__native_ref-java__1.1-natives.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-connector-all-shaded-spark_3.5_2.12--1532721621--http_proto-speed-src.jar:/databricks/jars/common--scalapb--scalapb-dynamic-runtime-conf-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--connector--iceberg--iceberg_shaded_deps--889402910--org.reactivestreams__reactive-streams__1.0.3.jar:/databricks/jars/daemon--data--client--conf--conf-spark_3.5_2.12_deploy.jar:/databricks/jars/macros--sourcecode--sourcecode-spark_3.5_2.12_deploy.jar:/databricks/jars/common--hadoop--hadoop-spark_3.5_2.12_deploy.jar:/databricks/jars/common--util--string-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--core--protobuf-java_shaded-for-proto_shade-hive-2.3__hadoop-3.2--1723257804--com.google.protobuf__protobuf-java__3.23.4.jar:/databricks/jars/armeria--conf--disable-client-usage-logging-rpc-metrics-spark_3.5_2.12_deploy.jar:/databricks/jars/chauffeur-api--api--endpoints--internal_proto_library-spark_3.5_2.12-scalabp.jar:/databricks/jars/proto--logs--settings-log--money_settings_log_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.sun.xml.bind--jaxb-core--com.sun.xml.bind__jaxb-core__2.2.11.jar:/databricks/jars/common--driver--replpool--repl_pool_common-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--connector--delta--server--delta_connect_server_lib_shaded-hive-2.3__hadoop-3.2_2.12---2077509976--libspark_connect_proto_lib_unshaded.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.amazonaws--aws-java-sdk-kms--com.amazonaws__aws-java-sdk-kms__1.12.610.jar:/databricks/jars/----ws_3_5--third_party--gcp-java--gcp-java_shaded---1562501062--com.google.apis__google-api-services-sts__v1beta-rev20201009-1.30.10.jar:/databricks/jars/spark--pipelines--common--common-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--third_party--mssql--mssql-hive-2.3__hadoop-3.2_2.12--1412259879--org.bouncycastle__bcprov-jdk15on__1.70.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.clearspring.analytics--stream--com.clearspring.analytics__stream__2.9.6.jar:/databricks/jars/common--logging--structured--catalog-spark_3.5_2.12_deploy.jar:/databricks/jars/common--conf--singleton--singleton-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--io.dropwizard.metrics--metrics-jetty9--io.dropwizard.metrics__metrics-jetty9__4.2.19.jar:/databricks/jars/common--logging--metrics--chauffeur--chauffeur-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.scalatest--scalatest-compatible--org.scalatest__scalatest-compatible__3.2.15.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.github.fommil.netlib--native_system-java--com.github.fommil.netlib__native_system-java__1.1.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.scalatest--scalatest-shouldmatchers_2.12--org.scalatest__scalatest-shouldmatchers_2.12__3.2.15.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--javax.xml.bind--jaxb-api--javax.xml.bind__jaxb-api__2.2.11.jar:/databricks/jars/----ws_3_5--connector--conduit--salesforce--source--salesforce_source-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/common--encryption--cpk-deps-shaded--1268522556--io.netty__netty-transport-native-kqueue__4.1.106.Final-osx-aarch_64.jar:/databricks/jars/common--util--guid-spark_3.5_2.12_deploy.jar:/databricks/jars/zippy--common--events--events-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.json4s--json4s-ast_2.12--org.json4s__json4s-ast_2.12__3.7.0-M11.jar:/databricks/jars/zippy--common--eventhandlers--eventhandlers-spark_3.5_2.12_deploy.jar:/databricks/jars/proto--logs--product-event--sql-edge--feature_usage_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/third_party--armeria--armeria_shaded_2.12--1596312874--io.grpc__grpc-stub__1.60.0.jar:/databricks/jars/armeria--conf--customize-grpc-client-dns-spark_3.5_2.12_deploy.jar:/databricks/jars/proto--logs--product-event--ml-algorithm--algorithm_proto-jetty9-hadoop1_2.12-scalabp.jar:/databricks/jars/proto--logs--settings-log--settings_log_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/----ws_3_5--connector--iceberg--iceberg_shaded_deps--1139128766--software.amazon.awssdk__aws-core__2.17.285.jar:/databricks/jars/third_party--armeria--armeria_shaded_2.12---1902266254--javax.annotation__javax.annotation-api__1.3.2.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.apache.curator--curator-framework--org.apache.curator__curator-framework__2.13.0.jar:/databricks/jars/common--conf--dp-to-cp-callback--static-dp-to-cp-client-config-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.apache.commons--commons-crypto--org.apache.commons__commons-crypto__1.1.0.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--javolution--javolution--javolution__javolution__5.5.1.jar:/databricks/jars/----ws_3_5--connector--aws-msk-iam-auth_shaded-for-kafka-0-10-hive-2.3__hadoop-3.2--1087773901--org.reactivestreams__reactive-streams__1.0.3.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---1084692028--io.netty__netty-codec-redis__4.1.106.Final.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.amazonaws--aws-java-sdk-ecs--com.amazonaws__aws-java-sdk-ecs__1.12.610.jar:/databricks/jars/common--database--datasource--standalone-spark_3.5_2.12_deploy.jar:/databricks/jars/common--jobs--run_id-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.google.flatbuffers--flatbuffers-java--com.google.flatbuffers__flatbuffers-java__23.5.26.jar:/databricks/jars/common--client--enable-tls-only-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--third_party--gcp-java--gcp-java_shaded--887796342--io.opencensus__opencensus-api__0.28.0.jar:/databricks/jars/zippy--common--cloudstorage--client--interface--interface-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--third_party--xgboost-predictor--xgboost-predictor-shaded--372974096--ai.h2o__h2o-tree-api__0.3.20.jar:/databricks/jars/proto--logs--activity--workspace-lifecycle-manager--workspace_lifecycle_manager_service_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/third_party--azure--azure-storage_shaded--966101332--com.microsoft.azure__azure-storage__8.6.6.jar:/databricks/jars/----ws_3_5--third_party--parquet-mr--parquet-format-structures--parquet-format-structures-shaded--561441283--libparquet-thrift.jar:/databricks/jars/----ws_3_5--connector--delta--server--delta_connect_server_lib_shaded-hive-2.3__hadoop-3.2_2.12--1463306973--com.google.errorprone__error_prone_annotations__2.10.0.jar:/databricks/jars/common--conf--static--static-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--io.dropwizard.metrics--metrics-core--io.dropwizard.metrics__metrics-core__4.2.19.jar:/databricks/jars/third_party--tink--tink-shaded_2.12---1154440303--io.netty__netty-tcnative-classes__2.0.61.Final.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded---1254772955--io.grpc__grpc-core__1.60.0.jar:/databricks/jars/----ws_3_5--third_party--gcp-java--gcp-java_shaded---839413769--com.google.cloud__google-cloud-notification__0.121.6-beta.jar:/databricks/jars/----ws_3_5--connector--connect--server--spark_connect_proto_lib_shaded-hive-2.3__hadoop-3.2_2.12--1596372094--io.grpc__grpc-util__1.60.0.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.microsoft.azure--azure-data-lake-store-sdk--com.microsoft.azure__azure-data-lake-store-sdk__2.3.9.jar:/databricks/jars/proto--logs--activity--brickindex--brickindex_activity_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded--999813863--com.google.errorprone__error_prone_annotations__2.10.0.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.tdunning--json--com.tdunning__json__1.8.jar:/databricks/jars/proto--logs--activity--auth--auth_service_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.amazonaws--aws-java-sdk-simpledb--com.amazonaws__aws-java-sdk-simpledb__1.12.610.jar:/databricks/jars/common--logging--log-level-control--log-level-control-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--third_party--gcp-java--gcp-java_shaded--831342450--io.grpc__grpc-stub__1.57.1.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.glassfish.hk2--hk2-locator--org.glassfish.hk2__hk2-locator__2.6.1.jar:/databricks/jars/third_party--opencensus-shaded--opencensus_shaded_base---229723244--com.google.j2objc__j2objc-annotations__2.8.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.apache.hadoop--hadoop-client-runtime--org.apache.hadoop__hadoop-client-runtime__3.3.6.jar:/databricks/jars/proto--logs--activity--marketplace--marketplace_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/key-manager--client--encryption-client-spark_3.5_2.12_deploy.jar:/databricks/jars/data-monitoring--api--api-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--third_party--bigquery-jdbc--bigquery-driver-shaded---846918551--google-http-client-1.43.1.jar:/databricks/jars/----ws_3_5--connector--connect--server--spark_connect_proto_lib_shaded-hive-2.3__hadoop-3.2_2.12---744856947--com.google.code.findbugs__jsr305__3.0.2.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--com.amazonaws--amazon-kinesis-client--com.amazonaws__amazon-kinesis-client__1.12.0.jar:/databricks/jars/----ws_3_5--connector--conduit--workdayraas--spark--workday-raas-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/third_party--armeria--armeria_shaded_2.12---1275080308--com.google.code.gson__gson__2.10.1.jar:/databricks/jars/third_party--armeria--armeria_shaded_2.12--458243678--com.google.protobuf__protobuf-java__3.25.3.jar:/databricks/jars/common--util--random-string-spark_3.5_2.12_deploy.jar:/databricks/jars/proto--logs--activity--cluster--cluster_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/common--logging--structured--proto-descriptor-util-spark_3.5_2.12_deploy.jar:/databricks/jars/servicemesh-control--endpoint-discovery-service--external--impl-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-connector-all-shaded-spark_3.5_2.12--1009477437--gcs-connector-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:/databricks/jars/proto--logs--product-event--auth--inhouse_oauth_token_endpoint_requests_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/----ws_3_5--connector--kafka-0-10-hive-2.3__hadoop-3.2_2.12_shaded--8073495--kafka-0-10-unshaded-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-connector-all-shaded-spark_3.5_2.12--1272494561--empty_proto-speed-src.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.slf4j--slf4j-simple--org.slf4j__slf4j-simple__1.7.25.jar:/databricks/jars/----ws_3_5--connector--delta--server--delta_connect_server_lib_shaded-hive-2.3__hadoop-3.2_2.12--1068632454--com.google.android__annotations__4.1.1.4.jar:/databricks/jars/----ws_3_5--third_party--gcp-java--gcp-java_shaded--1841609185--com.google.http-client__google-http-client__1.42.3.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---2116221318--io.grpc__grpc-alts__1.50.2.jar:/databricks/jars/----ws_3_5--third_party--pipelines--pipelines-sdk--1797194586--io.github.classgraph__classgraph__4.8.78.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12--1428008859--com.fasterxml.jackson.core__jackson-annotations__2.14.2.jar:/databricks/jars/logging--utils--logging-utils-spark_3.5_2.12_deploy.jar:/databricks/jars/common--database--querylogger--querylogger-spark_3.5_2.12_deploy.jar:/databricks/jars/common--user--user-spark_3.5_2.12_deploy.jar:/databricks/jars/common--jetty--client--client-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--grpc-netty--grpc-netty_shaded--103420347--io.netty__netty-codec-mqtt__4.1.106.Final.jar:/databricks/jars/common--logging--metrics--runtime--runtime-spark_3.5_2.12_deploy.jar:/databricks/jars/proto--logs--activity--unitycatalog--unitycatalog_metastore_mover_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/----ws_3_5--common--tags--tags-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----ws_3_5--third_party--bigquery-jdbc--bigquery-driver-shaded---846918551--grpc-auth-1.54.0.jar:/databricks/jars/common--rpc--formats--proto--formats_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/common--hadoop--filesystem--loki-filesystem-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---2126672086--io.opencensus__opencensus-api__0.24.0.jar:/databricks/jars/----ws_3_5--connector--aws-msk-iam-auth_shaded-for-kafka-0-10-hive-2.3__hadoop-3.2---1315685466--software.amazon.awssdk__apache-client__2.20.121.jar:/databricks/jars/common--rpc--checksum--checksum-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--third_party--jts--jts-shaded---1018319394--com.googlecode.json-simple__json-simple__1.1.1.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--io.netty--netty-tcnative-boringssl-static--io.netty__netty-tcnative-boringssl-static__2.0.61.Final-linux-x86_64.jar:/databricks/jars/common--rpc--metrics--tls_metrics-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--connector--delta--server--delta_connect_server_lib_shaded-hive-2.3__hadoop-3.2_2.12--1920349914--io.grpc__grpc-services__1.60.0.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded--785318626--com.google.auth__google-auth-library-oauth2-http__1.4.0.jar:/databricks/jars/----ws_3_5--connector--avro--avro_resources_shaded-for-avro-hive-2.3__hadoop-3.2---606136534--libavro_resources.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--org.apache.ant--ant-jsch--org.apache.ant__ant-jsch__1.10.11.jar:/databricks/jars/third_party--armeria--armeria_shaded_2.12---1544760773--io.micrometer__micrometer-core__1.8.5.jar:/databricks/jars/common--rpc--util--util-spark_3.5_2.12_deploy.jar:/databricks/jars/spark--dbutils--dbutils-api-spark_3.5_2.12_deploy.jar:/databricks/jars/proto--logs--sla--ticpapiserver--ticp_events_extra_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-connector-all-shaded-spark_3.5_2.12--1532721621--libclient_proto-speed.jar:/databricks/jars/----ws_3_5--third_party--gcp-java--gcp-java_shaded---1207969205--com.google.api.grpc__proto-google-iam-v1__1.0.14.jar:/databricks/jars/third_party--tink--tink-shaded_2.12--218144780--com.google.code.findbugs__jsr305__3.0.2.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded---1144976963--com.google.flogger__google-extensions__0.5.1.jar:/databricks/jars/----ws_3_5--third_party--parquet-mr--parquet-hadoop--parquet-hadoop-shaded---358486728--libparquet-hadoop-internal.jar:/databricks/jars/----ws_3_5--mvn--hadoop3--io.netty--netty-tcnative-boringssl-static--io.netty__netty-tcnative-boringssl-static__2.0.61.Final-osx-x86_64.jar:/databricks/jars/proto--logs--activity--data-sharing--data_sharing_service_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/iam-common--dali--api--api-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--connector--delta--server--delta_connect_server_lib_shaded-hive-2.3__hadoop-3.2_2.12---330209066--io.netty__netty-codec__4.1.106.Final.jar:/databricks/jars/spark--driver--abtesting--client-spark_3.5_2.12_deploy.jar:/databricks/jars/----ws_3_5--third_party--kinesis-sdk--kinesis-sdk--1558491337--software.amazon.awssdk__auth__2.17.190.jar:/databricks/jars/third_party--tink--tink-shaded_2.12--1494603536--com.lihaoyi__sourcecode_2.12__0.1.7.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12--861601547--com.google.errorprone__error_prone_annotations__2.18.0.jar:/databricks/jars/secret-manager--helper--helper-spark_3.5_2.12_deploy.jar:/databricks/jars/proto--logs--product-event--spark-operation--spark_stage_completed_proto-jetty9-hadoop1_2.12-scalabp.jar:/databricks/jars/----glue-catalog-spark3.5-client--glue-catalog-shim-hive2-jetty9-hadoop1_2.12_deploy.jar:/databricks/jars/common--encryption--cpk-deps-shaded--1980642199--io.netty__netty-tcnative-boringssl-static__2.0.61.Final-db-r14-osx-x86_64.jar:/databricks/jars/proto--logs--product-event--auth--common_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/common--context--integrity--integrity-check-context-spark_3.5_2.12_deploy.jar:/databricks/jars/feature-flag--client--common-spark_3.5_2.12_deploy.jar:/databricks/jars/proto--logs--serverless-slot-based-billing--serverless_slot_based_billing_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12--1551017351--com.fasterxml.jackson.datatype__jackson-datatype-joda__2.9.8.jar:/databricks/jars/common--event-id--event-id-utils-spark_3.5_2.12_deploy.jar:/databricks/jars/third_party--hadoop_gcs--hadoop-connectors--gcs-dependencies-shaded--19273035--com.google.flogger__flogger__0.5.1.jar:/databricks/jars/proto--logs--activity--search-midtier--search_midtier_service_proto-spark_3.5_2.12-scalabp.jar:/databricks/jars/third_party--jetty-client--jetty-client--411691605--org.eclipse.jetty__jetty-client__9.4.51.v20230217.jar:/databricks/jars/----ws_3_5--sql--hive-thriftserver--hive-thriftserver-protocol-hive-2.3__hadoop-3.2_2.12_deploy.jar:/databricks/jars/----ws_3_5--connector--delta--server--delta_connect_server_lib_shaded-hive-2.3__hadoop-3.2_2.12--1595831323--io.grpc__grpc-core__1.60.0.jar:/databricks/jars/third_party--tink--tink-shaded_2.12--1462658334--com.aayushatharva.brotli4j__brotli4j__1.7.1.jar:/databricks/jars/third_party--hadoop-azure-2.7.3-abfs--hadoop-azure-2.7.3-abfs-external-shaded_20180625_3682417-spark_3.5_2.12--747048218--io.netty__netty-all__4.1.94.Final.jar:/databricks/jars/----ws_3_5--sql--api--libsql-api_resources.jar:/databricks/jars/third_party--armeria--armeria_shaded_2.12--598497736--io.netty__netty-transport-native-epoll__4.1.106.Final-linux-x86_64.jar:/databricks/jars/----ws_3_5--connector--connect--server--spark_connect_proto_lib_shaded-hive-2.3__hadoop-3.2_2.12--1572094211--com.google.code.findbug\n\n*** WARNING: max output size exceeded, skipping output. ***\n\nype = producer\n\tcompression.zstd.level = 3\n\tconnection.failed.authentication.delay.ms = 100\n\tconnections.max.idle.ms = 600000\n\tconnections.max.reauth.ms = 0\n\tcontrol.plane.listener.name = null\n\tcontrolled.shutdown.enable = true\n\tcontrolled.shutdown.max.retries = 3\n\tcontrolled.shutdown.retry.backoff.ms = 5000\n\tcontroller.listener.names = null\n\tcontroller.quorum.append.linger.ms = 25\n\tcontroller.quorum.bootstrap.servers = []\n\tcontroller.quorum.election.backoff.max.ms = 1000\n\tcontroller.quorum.election.timeout.ms = 1000\n\tcontroller.quorum.fetch.timeout.ms = 2000\n\tcontroller.quorum.request.timeout.ms = 2000\n\tcontroller.quorum.retry.backoff.ms = 20\n\tcontroller.quorum.voters = []\n\tcontroller.quota.window.num = 11\n\tcontroller.quota.window.size.seconds = 1\n\tcontroller.socket.timeout.ms = 30000\n\tcreate.topic.policy.class.name = null\n\tdefault.replication.factor = 1\n\tdelegation.token.expiry.check.interval.ms = 3600000\n\tdelegation.token.expiry.time.ms = 86400000\n\tdelegation.token.master.key = null\n\tdelegation.token.max.lifetime.ms = 604800000\n\tdelegation.token.secret.key = null\n\tdelete.records.purgatory.purge.interval.requests = 1\n\tdelete.topic.enable = true\n\tearly.start.listeners = null\n\teligible.leader.replicas.enable = false\n\tfetch.max.bytes = 57671680\n\tfetch.purgatory.purge.interval.requests = 1000\n\tgroup.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]\n\tgroup.consumer.heartbeat.interval.ms = 5000\n\tgroup.consumer.max.heartbeat.interval.ms = 15000\n\tgroup.consumer.max.session.timeout.ms = 60000\n\tgroup.consumer.max.size = 2147483647\n\tgroup.consumer.migration.policy = disabled\n\tgroup.consumer.min.heartbeat.interval.ms = 5000\n\tgroup.consumer.min.session.timeout.ms = 45000\n\tgroup.consumer.session.timeout.ms = 45000\n\tgroup.coordinator.append.linger.ms = 10\n\tgroup.coordinator.new.enable = false\n\tgroup.coordinator.rebalance.protocols = [classic]\n\tgroup.coordinator.threads = 1\n\tgroup.initial.rebalance.delay.ms = 0\n\tgroup.max.session.timeout.ms = 1800000\n\tgroup.max.size = 2147483647\n\tgroup.min.session.timeout.ms = 6000\n\tinitial.broker.registration.timeout.ms = 60000\n\tinter.broker.listener.name = null\n\tinter.broker.protocol.version = 3.8-IV0\n\tkafka.metrics.polling.interval.secs = 10\n\tkafka.metrics.reporters = []\n\tleader.imbalance.check.interval.seconds = 300\n\tleader.imbalance.per.broker.percentage = 10\n\tlistener.security.protocol.map = SASL_SSL:SASL_SSL,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT\n\tlisteners = PLAINTEXT://:9092\n\tlog.cleaner.backoff.ms = 15000\n\tlog.cleaner.dedupe.buffer.size = 134217728\n\tlog.cleaner.delete.retention.ms = 86400000\n\tlog.cleaner.enable = true\n\tlog.cleaner.io.buffer.load.factor = 0.9\n\tlog.cleaner.io.buffer.size = 524288\n\tlog.cleaner.io.max.bytes.per.second = 1.7976931348623157E308\n\tlog.cleaner.max.compaction.lag.ms = 9223372036854775807\n\tlog.cleaner.min.cleanable.ratio = 0.5\n\tlog.cleaner.min.compaction.lag.ms = 0\n\tlog.cleaner.threads = 1\n\tlog.cleanup.policy = [delete]\n\tlog.dir = /tmp/kafka-logs\n\tlog.dir.failure.timeout.ms = 30000\n\tlog.dirs = /tmp/kafka-logs\n\tlog.flush.interval.messages = 9223372036854775807\n\tlog.flush.interval.ms = null\n\tlog.flush.offset.checkpoint.interval.ms = 60000\n\tlog.flush.scheduler.interval.ms = 9223372036854775807\n\tlog.flush.start.offset.checkpoint.interval.ms = 60000\n\tlog.index.interval.bytes = 4096\n\tlog.index.size.max.bytes = 10485760\n\tlog.initial.task.delay.ms = 30000\n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tlog.message.downconversion.enable = true\n\tlog.message.format.version = 3.0-IV1\n\tlog.message.timestamp.after.max.ms = 9223372036854775807\n\tlog.message.timestamp.before.max.ms = 9223372036854775807\n\tlog.message.timestamp.difference.max.ms = 9223372036854775807\n\tlog.message.timestamp.type = CreateTime\n\tlog.preallocate = false\n\tlog.retention.bytes = -1\n\tlog.retention.check.interval.ms = 300000\n\tlog.retention.hours = 168\n\tlog.retention.minutes = null\n\tlog.retention.ms = null\n\tlog.roll.hours = 168\n\tlog.roll.jitter.hours = 0\n\tlog.roll.jitter.ms = null\n\tlog.roll.ms = null\n\tlog.segment.bytes = 1073741824\n\tlog.segment.delete.delay.ms = 60000\n\tmax.connection.creation.rate = 2147483647\n\tmax.connections = 2147483647\n\tmax.connections.per.ip = 2147483647\n\tmax.connections.per.ip.overrides = \n\tmax.incremental.fetch.session.cache.slots = 1000\n\tmax.request.partition.size.limit = 2000\n\tmessage.max.bytes = 1048588\n\tmetadata.log.dir = null\n\tmetadata.log.max.record.bytes.between.snapshots = 20971520\n\tmetadata.log.max.snapshot.interval.ms = 3600000\n\tmetadata.log.segment.bytes = 1073741824\n\tmetadata.log.segment.min.bytes = 8388608\n\tmetadata.log.segment.ms = 604800000\n\tmetadata.max.idle.interval.ms = 500\n\tmetadata.max.retention.bytes = 104857600\n\tmetadata.max.retention.ms = 604800000\n\tmetric.reporters = []\n\tmetrics.num.samples = 2\n\tmetrics.recording.level = INFO\n\tmetrics.sample.window.ms = 30000\n\tmin.insync.replicas = 1\n\tnode.id = 0\n\tnum.io.threads = 8\n\tnum.network.threads = 3\n\tnum.partitions = 1\n\tnum.recovery.threads.per.data.dir = 1\n\tnum.replica.alter.log.dirs.threads = null\n\tnum.replica.fetchers = 1\n\toffset.metadata.max.bytes = 4096\n\toffsets.commit.required.acks = -1\n\toffsets.commit.timeout.ms = 5000\n\toffsets.load.buffer.size = 5242880\n\toffsets.retention.check.interval.ms = 600000\n\toffsets.retention.minutes = 10080\n\toffsets.topic.compression.codec = 0\n\toffsets.topic.num.partitions = 50\n\toffsets.topic.replication.factor = 1\n\toffsets.topic.segment.bytes = 104857600\n\tpassword.encoder.cipher.algorithm = AES/CBC/PKCS5Padding\n\tpassword.encoder.iterations = 4096\n\tpassword.encoder.key.length = 128\n\tpassword.encoder.keyfactory.algorithm = null\n\tpassword.encoder.old.secret = null\n\tpassword.encoder.secret = null\n\tprincipal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder\n\tprocess.roles = []\n\tproducer.id.expiration.check.interval.ms = 600000\n\tproducer.id.expiration.ms = 86400000\n\tproducer.purgatory.purge.interval.requests = 1000\n\tqueued.max.request.bytes = -1\n\tqueued.max.requests = 500\n\tquota.window.num = 11\n\tquota.window.size.seconds = 1\n\tremote.fetch.max.wait.ms = 500\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.copier.thread.pool.size = 10\n\tremote.log.manager.copy.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.copy.quota.window.num = 11\n\tremote.log.manager.copy.quota.window.size.seconds = 1\n\tremote.log.manager.expiration.thread.pool.size = 10\n\tremote.log.manager.fetch.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.fetch.quota.window.num = 11\n\tremote.log.manager.fetch.quota.window.size.seconds = 1\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n\treplica.fetch.backoff.ms = 1000\n\treplica.fetch.max.bytes = 1048576\n\treplica.fetch.min.bytes = 1\n\treplica.fetch.response.max.bytes = 10485760\n\treplica.fetch.wait.max.ms = 500\n\treplica.high.watermark.checkpoint.interval.ms = 5000\n\treplica.lag.time.max.ms = 30000\n\treplica.selector.class = null\n\treplica.socket.receive.buffer.bytes = 65536\n\treplica.socket.timeout.ms = 30000\n\treplication.quota.window.num = 11\n\treplication.quota.window.size.seconds = 1\n\trequest.timeout.ms = 30000\n\treserved.broker.max.id = 1000\n\tsasl.client.callback.handler.class = null\n\tsasl.enabled.mechanisms = [GSSAPI]\n\tsasl.jaas.config = null\n\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n\tsasl.kerberos.min.time.before.relogin = 60000\n\tsasl.kerberos.principal.to.local.rules = [DEFAULT]\n\tsasl.kerberos.service.name = null\n\tsasl.kerberos.ticket.renew.jitter = 0.05\n\tsasl.kerberos.ticket.renew.window.factor = 0.8\n\tsasl.login.callback.handler.class = null\n\tsasl.login.class = null\n\tsasl.login.connect.timeout.ms = null\n\tsasl.login.read.timeout.ms = null\n\tsasl.login.refresh.buffer.seconds = 300\n\tsasl.login.refresh.min.period.seconds = 60\n\tsasl.login.refresh.window.factor = 0.8\n\tsasl.login.refresh.window.jitter = 0.05\n\tsasl.login.retry.backoff.max.ms = 10000\n\tsasl.login.retry.backoff.ms = 100\n\tsasl.mechanism.controller.protocol = GSSAPI\n\tsasl.mechanism.inter.broker.protocol = GSSAPI\n\tsasl.oauthbearer.clock.skew.seconds = 30\n\tsasl.oauthbearer.expected.audience = null\n\tsasl.oauthbearer.expected.issuer = null\n\tsasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000\n\tsasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100\n\tsasl.oauthbearer.jwks.endpoint.url = null\n\tsasl.oauthbearer.scope.claim.name = scope\n\tsasl.oauthbearer.sub.claim.name = sub\n\tsasl.oauthbearer.token.endpoint.url = null\n\tsasl.server.callback.handler.class = null\n\tsasl.server.max.receive.size = 524288\n\tsecurity.inter.broker.protocol = PLAINTEXT\n\tsecurity.providers = null\n\tserver.max.startup.time.ms = 9223372036854775807\n\tsocket.connection.setup.timeout.max.ms = 30000\n\tsocket.connection.setup.timeout.ms = 10000\n\tsocket.listen.backlog.size = 50\n\tsocket.receive.buffer.bytes = 102400\n\tsocket.request.max.bytes = 104857600\n\tsocket.send.buffer.bytes = 102400\n\tssl.allow.dn.changes = false\n\tssl.allow.san.changes = false\n\tssl.cipher.suites = []\n\tssl.client.auth = none\n\tssl.enabled.protocols = [TLSv1.2]\n\tssl.endpoint.identification.algorithm = https\n\tssl.engine.factory.class = null\n\tssl.key.password = null\n\tssl.keymanager.algorithm = SunX509\n\tssl.keystore.certificate.chain = null\n\tssl.keystore.key = null\n\tssl.keystore.location = null\n\tssl.keystore.password = null\n\tssl.keystore.type = JKS\n\tssl.principal.mapping.rules = DEFAULT\n\tssl.protocol = TLSv1.2\n\tssl.provider = null\n\tssl.secure.random.implementation = null\n\tssl.trustmanager.algorithm = PKIX\n\tssl.truststore.certificates = null\n\tssl.truststore.location = null\n\tssl.truststore.password = null\n\tssl.truststore.type = JKS\n\ttelemetry.max.bytes = 1048576\n\ttransaction.abort.timed.out.transaction.cleanup.interval.ms = 10000\n\ttransaction.max.timeout.ms = 900000\n\ttransaction.partition.verification.enable = true\n\ttransaction.remove.expired.transaction.cleanup.interval.ms = 3600000\n\ttransaction.state.log.load.buffer.size = 5242880\n\ttransaction.state.log.min.isr = 1\n\ttransaction.state.log.num.partitions = 50\n\ttransaction.state.log.replication.factor = 1\n\ttransaction.state.log.segment.bytes = 104857600\n\ttransactional.id.expiration.ms = 604800000\n\tunclean.leader.election.enable = false\n\tunstable.api.versions.enable = false\n\tunstable.feature.versions.enable = false\n\tzookeeper.clientCnxnSocket = null\n\tzookeeper.connect = localhost:2181\n\tzookeeper.connection.timeout.ms = 18000\n\tzookeeper.max.in.flight.requests = 10\n\tzookeeper.metadata.migration.enable = false\n\tzookeeper.metadata.migration.min.batch.size = 200\n\tzookeeper.session.timeout.ms = 18000\n\tzookeeper.set.acl = false\n\tzookeeper.ssl.cipher.suites = null\n\tzookeeper.ssl.client.enable = false\n\tzookeeper.ssl.crl.enable = false\n\tzookeeper.ssl.enabled.protocols = null\n\tzookeeper.ssl.endpoint.identification.algorithm = HTTPS\n\tzookeeper.ssl.keystore.location = null\n\tzookeeper.ssl.keystore.password = null\n\tzookeeper.ssl.keystore.type = null\n\tzookeeper.ssl.ocsp.enable = false\n\tzookeeper.ssl.protocol = TLSv1.2\n\tzookeeper.ssl.truststore.location = null\n\tzookeeper.ssl.truststore.password = null\n\tzookeeper.ssl.truststore.type = null\n (kafka.server.KafkaConfig)\n[2024-10-23 15:50:08,619] INFO RemoteLogManagerConfig values: \n\tlog.local.retention.bytes = -2\n\tlog.local.retention.ms = -2\n\tremote.fetch.max.wait.ms = 500\n\tremote.log.index.file.cache.total.size.bytes = 1073741824\n\tremote.log.manager.copier.thread.pool.size = 10\n\tremote.log.manager.copy.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.copy.quota.window.num = 11\n\tremote.log.manager.copy.quota.window.size.seconds = 1\n\tremote.log.manager.expiration.thread.pool.size = 10\n\tremote.log.manager.fetch.max.bytes.per.second = 9223372036854775807\n\tremote.log.manager.fetch.quota.window.num = 11\n\tremote.log.manager.fetch.quota.window.size.seconds = 1\n\tremote.log.manager.task.interval.ms = 30000\n\tremote.log.manager.task.retry.backoff.max.ms = 30000\n\tremote.log.manager.task.retry.backoff.ms = 500\n\tremote.log.manager.task.retry.jitter = 0.2\n\tremote.log.manager.thread.pool.size = 10\n\tremote.log.metadata.custom.metadata.max.bytes = 128\n\tremote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager\n\tremote.log.metadata.manager.class.path = null\n\tremote.log.metadata.manager.impl.prefix = rlmm.config.\n\tremote.log.metadata.manager.listener.name = null\n\tremote.log.reader.max.pending.tasks = 100\n\tremote.log.reader.threads = 10\n\tremote.log.storage.manager.class.name = null\n\tremote.log.storage.manager.class.path = null\n\tremote.log.storage.manager.impl.prefix = rsm.config.\n\tremote.log.storage.system.enable = false\n (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)\n[2024-10-23 15:50:10,117] INFO [ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2024-10-23 15:50:10,142] INFO [ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2024-10-23 15:50:10,170] INFO [ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2024-10-23 15:50:10,255] INFO [ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)\n[2024-10-23 15:50:10,505] INFO [KafkaServer id=0] Rewriting /tmp/kafka-logs/meta.properties (kafka.server.KafkaServer)\n[2024-10-23 15:50:11,664] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)\n[2024-10-23 15:50:11,826] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)\n[2024-10-23 15:50:12,145] INFO Loaded 0 logs in 472ms (kafka.log.LogManager)\n[2024-10-23 15:50:12,371] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)\n[2024-10-23 15:50:12,385] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)\n[2024-10-23 15:50:13,422] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)\n[2024-10-23 15:50:13,798] INFO [feature-zk-node-event-process-thread]: Starting (kafka.server.FinalizedFeatureChangeListener$ChangeNotificationProcessorThread)\n[2024-10-23 15:50:13,960] INFO Feature ZK node at path: /feature does not exist (kafka.server.FinalizedFeatureChangeListener)\n[2024-10-23 15:50:14,930] INFO [zk-broker-0-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2024-10-23 15:50:23,708] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)\n[2024-10-23 15:50:23,862] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)\n[2024-10-23 15:50:23,935] INFO [zk-broker-0-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)\n[2024-10-23 15:50:24,266] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-23 15:50:24,275] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-23 15:50:24,281] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-23 15:50:24,294] INFO [ExpirationReaper-0-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-23 15:50:24,303] INFO [ExpirationReaper-0-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-23 15:50:24,529] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)\n[2024-10-23 15:50:24,543] INFO [AddPartitionsToTxnSenderThread-0]: Starting (kafka.server.AddPartitionsToTxnManager)\n[2024-10-23 15:50:24,764] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)\n[2024-10-23 15:50:25,105] INFO Stat of the created znode at /brokers/ids/0 is: 25,25,1729698625004,1729698625004,1,0,0,72058342631669760,252,0,25\n (kafka.zk.KafkaZkClient)\n[2024-10-23 15:50:25,117] INFO Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://1023-154143-zl92n0lo-10-172-187-10:9092, czxid (broker epoch): 25 (kafka.zk.KafkaZkClient)\n[2024-10-23 15:50:26,213] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-23 15:50:26,357] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-23 15:50:26,362] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-23 15:50:26,437] INFO Successfully created /controller_epoch with initial epoch 0 (kafka.zk.KafkaZkClient)\n[2024-10-23 15:50:26,582] INFO Feature ZK node created at path: /feature (kafka.server.FinalizedFeatureChangeListener)\n[2024-10-23 15:50:26,664] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)\n[2024-10-23 15:50:26,712] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)\n[2024-10-23 15:50:26,908] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)\n[2024-10-23 15:50:26,917] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)\n[2024-10-23 15:50:26,924] INFO [TxnMarkerSenderThread-0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)\n[2024-10-23 15:50:26,933] INFO [MetadataCache brokerId=0] Updated cache from existing None to latest Features(metadataVersion=3.8-IV0, finalizedFeatures={}, finalizedFeaturesEpoch=0). (kafka.server.metadata.ZkMetadataCache)\n[2024-10-23 15:50:27,833] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:27,834] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)\n[2024-10-23 15:50:27,873] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1023-154143-zl92n0lo-10-172-187-10/127.0.1.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:27,963] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:28,094] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:28,094] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1023-154143-zl92n0lo-10-172-187-10/127.0.1.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:28,095] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:28,196] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:28,196] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1023-154143-zl92n0lo-10-172-187-10/127.0.1.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:28,197] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:28,298] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:28,298] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1023-154143-zl92n0lo-10-172-187-10/127.0.1.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:28,299] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:28,400] INFO [Controller id=0, targetBrokerId=0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:28,400] WARN [Controller id=0, targetBrokerId=0] Connection to node 0 (1023-154143-zl92n0lo-10-172-187-10/127.0.1.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:28,400] INFO [Controller id=0, targetBrokerId=0] Client requested connection close from node 0 (org.apache.kafka.clients.NetworkClient)\n[2024-10-23 15:50:28,433] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)\n[2024-10-23 15:50:28,450] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Enabling request processing. (kafka.network.SocketServer)\n[2024-10-23 15:50:28,459] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)\n[2024-10-23 15:50:28,550] INFO [KafkaServer id=0] Start processing authorizer futures (kafka.server.KafkaServer)\n[2024-10-23 15:50:28,551] INFO [KafkaServer id=0] End processing authorizer futures (kafka.server.KafkaServer)\n[2024-10-23 15:50:28,555] INFO [KafkaServer id=0] Start processing enable request processing future (kafka.server.KafkaServer)\n[2024-10-23 15:50:28,555] INFO [KafkaServer id=0] End processing enable request processing future (kafka.server.KafkaServer)\n[2024-10-23 15:50:28,591] INFO Kafka version: 7.4.0-ccs (org.apache.kafka.common.utils.AppInfoParser)\n[2024-10-23 15:50:28,593] INFO Kafka commitId: 30969fa33c185e88 (org.apache.kafka.common.utils.AppInfoParser)\n[2024-10-23 15:50:28,594] INFO Kafka startTimeMs: 1729698628556 (org.apache.kafka.common.utils.AppInfoParser)\n[2024-10-23 15:50:28,613] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)\n[2024-10-23 15:50:29,606] INFO [zk-broker-0-to-controller-forwarding-channel-manager]: Recorded new ZK controller, from now on will use node 1023-154143-zl92n0lo-10-172-187-10:9092 (id: 0 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2024-10-23 15:50:29,606] INFO [zk-broker-0-to-controller-alter-partition-channel-manager]: Recorded new ZK controller, from now on will use node 1023-154143-zl92n0lo-10-172-187-10:9092 (id: 0 rack: null) (kafka.server.NodeToControllerRequestThread)\n[2024-10-23 15:50:33,654] INFO Creating topic genomics-news with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)\n[2024-10-23 15:50:33,988] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(genomics-news-0) (kafka.server.ReplicaFetcherManager)\n[2024-10-23 15:50:34,223] INFO [LogLoader partition=genomics-news-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2024-10-23 15:50:34,274] INFO Created log for partition genomics-news-0 in /tmp/kafka-logs/genomics-news-0 with properties {} (kafka.log.LogManager)\n[2024-10-23 15:50:34,278] INFO [Partition genomics-news-0 broker=0] No checkpointed highwatermark is found for partition genomics-news-0 (kafka.cluster.Partition)\n[2024-10-23 15:50:34,284] INFO [Partition genomics-news-0 broker=0] Log loaded for partition genomics-news-0 with initial high watermark 0 (kafka.cluster.Partition)\n[2024-10-23 15:50:45,343] INFO Creating topic clean-data with configuration {} and initial partition assignment Map(0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)\n[2024-10-23 15:50:45,385] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(clean-data-0) (kafka.server.ReplicaFetcherManager)\n[2024-10-23 15:50:45,398] INFO [LogLoader partition=clean-data-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)\n[2024-10-23 15:50:45,403] INFO Created log for partition clean-data-0 in /tmp/kafka-logs/clean-data-0 with properties {} (kafka.log.LogManager)\n[2024-10-23 15:50:45,406] INFO [Partition clean-data-0 broker=0] No checkpointed highwatermark is found for partition clean-data-0 (kafka.cluster.Partition)\n[2024-10-23 15:50:45,408] INFO [Partition clean-data-0 broker=0] Log loaded for partition clean-data-0 with initial high watermark 0 (kafka.cluster.Partition)\n"
     ]
    }
   ],
   "source": [
    "%sh \n",
    "/dbfs/projeto/kafka_2.12-3.8.0/bin/kafka-server-start.sh /dbfs/projeto/kafka_2.12-3.8.0/config/server.properties"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 898858298568824,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02.setup_kafka_server",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
